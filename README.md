## Формат на дърво
На първия ред пише броя върхове. Следват n-1 реда с двойки върхове - ребрата между върховете. Върховете са
enumerate-нати от 0 до n-1. Причина за използване на такъв формат:
* удобно за работа
* лесно се plot-ва (<https://csacademy.com/app/graph_editor/>)
* малък overhead откъм памет, но дърветата реалистично няма да са с повече от 100 върха.

## Backtracking (variable ordering)
Използвам backtracking search, за да реша проблема. Първото важно нещо в такъв тип алгоритъми е да
изберем node ordering. Следвам правилото most restrained variable (MRV) и ще избирам тези върхове,
който имат най-малък брой валидни стойности. Ако до момента съм сложил (валидни) стойности на
върховете V' и остава да оценя V\V', то задължително ще избера връх, който има съсед в V'. Ако не
избера такъв връх, backtracking-а се редуцира до пълно обхождане. Тази стратегия ми гарантира почти
оптимално следване на MRV без допълнителни изчисления.

> Избирам начален връх за корен на дървото. Избирам статична наредба на върховете, която да е
> топологично сортиране на дървото.

Статичната наредба забързва сметките и емпирично води до много по-добри резултати (спрямо динамична).

## Backtracking (value ordering)
След като изберем връх, трябва да му изберем оценка. За целта пазим информация за срещаните разлики
и за използваните стойности за въхове. Искаме да изберем такава стойност, такава че да не се среща и
такава, че разликата между нея и родителя й да е уникална. Тук статичната наредба ни улеснява да
сверяваме само с родителя си. Също така искаме да изберем такава стойност, който най-вероятно ще
ни отведе до решение (least constraining value). Как може да определим това?

## Data exploration
Направих анализ на дърветата с 9 върха (TLDR at bottom), където корена сложих да е със стойност 1. За
всеки връх може да разгледаме всяка негова стойност до колко решения води. Така получаваме псевдо random
variable (даден връх), където P(X=x) е броя решения, при които върхът X има стойност x, разделен на общия
брой решения. За X търсим такива x, че P(X=x) да е високо:
* избиране на корен с най-голям degree води до по-добри резултати
* върховете близо до корена следват pattern за стойностите си - ако са на нечетна дълбочина, е много
по-оптимално да са с големи стойности (има много по-голям брой решения). Ако са на четна дълбочина се
обръща ефекта - важно е да имат малка стойност.
* този pattern намаля с отдалечаване от корена. той е и повлиян от височината. Например, ако вземем съсед на
корена, който е листо, ще е много по-слабо изразен от другите му съседи, но все пак по-силно изразен от
произволно листо
* ако намалее достатъчно този pattern е по-добре да пробваме стойности на random.
#### Обяснение
Ако си припомним клипчето от yt, където говориха за линейни дървета, там решение винаги има, ако се редуват
големи-малки стойности. Обратно, ако имаме голяма разклоненост при даден връх, за всичи негови деца ще сложим
малки стойности. Това означава, че запълваме големите разлики, който са най-трудни за запълване. Напр.
най-голямата разлика се получава единствено, ако най-голямия и най-малкия връх са съседи.

#### TLDR
* харесваме високи стойности за върховете с нечетна дълбочина
* харесваме ниски стойности за върховете с четна дълбочина
* горния ефект fade-ва постепенно докато не е по-добре да харесваме random стойности
* колко е засилен ефекта може да се изчислим с machine learning

## Machine Learning
От горните съображения кандидат feature-и за даден връх u са
1. дълбочина
2. височина
3. брой node-ове в поддървото, вкоренено в u

Понеже ще тренираме с малък вход (графи с 9-14 върха), трябва резултатите да не зависят от n. Добавяме:

4. брой върхове, например за да scale-нем feature 3
5. средна дълбочна/височина за графа
6. максимална дълбочина/височина за графа

Тук проблемът е, че сме ограничени с графи до 14-15 върха, понеже трябва да обходим всички решения, за да
тренираме. Априори това няма да проработи, но практически се справя добре. Какво ще предсказваме? От горния
random variable X може да пресметнем очакването за брой стъпки, докато намерим решение. Първата стратегия е да
почнем в даден край, т.е. E1 = P(X=1) + 2*P(X=3) + ... Другия вариант е да избираме на random стойности. Нека да 
приемем, че X е равномерно разпределен и така очакването ще е E2 = n/2. Колкото по-малко е първото очакване от
n/2, толкова по-добре. Взимаме relative error за оценка колко е засилен ефекта: (E2 - E1) / E2.

За малкото feature-и, който сме избрали, RandomForestRegressor работи перфектно. Правя минимален feature engineering.
Оставил съм notebook да покажа как тренирам. В крайна сметка с модела получаваме за всеки връх приближен горния relative error,
който се намира в интервала [-1, 1]. Ще кръстим тези числа stability.

> Според stability определяме стратегия за даване на стойности на върховете. Ако е малко над 0, редуваме ниски-високи.
> В противен случай правим random order.

Вероятно всяко дърво има решение. Нашия алгоритъм ще приоритизира да търси решения. Затова правим лека модификация.
> Топологичното сортиране ще е такова, че най-стабилините върхове да са най-напред в подредбата.
> Така в началните върхове ще правим най-малко backtracking, при които е най-скъпо.

## Резултати
Оценката на алгоритъма: имаме 5 секунди да намерим решение. Ако в повече от 10% от случаите не
успеем да намерим решение, дървото е твърде голямо.

* Без да прилагаме стратегията с редуването: горен лимит от 14-15 node-a.
* Винаги редуваме, но без machine learning (т.е. произволен topooder): горен лимит от 30 node-a.
* Винаги редуваме и с machine learning (т.е. теоретично добър topoorder): горен лимит до 70 node-a.
* Adaptive редуване/random order: лимит от 75 node-a.

#### Заключения първо
Random is not good enough. Затова за по-лесно оставаме винаги да се редува. Това ще ни позволява да направим
оптимизация. see below.

#### Заключение две
Реално програмата работи за 0.1 секунди или за (вероятно много) повече от 5. Затова би било добра идея да въведем
local search в csp-то. Например, ако до 0.1 секунди не намери решения, да се откаже и да почне наново. Simulated annealing
с random restart е go-to опция, понеже вече имаме изчислен stability. Не съм тествал дали и как би работило, понеже условието
изисква да може да пресметнем дали няма решение, в случай, че няма. Но предполагам, че би работило малко по-добре.

#### Mini optimization
Понеже редуваме големи-малки стойности, може в backtracker-а да пазим две променливи nextMin, nextMax,
който ни казват кой е най-малкия/най-големия елемент, който не е зает до момента от никой връх. Така, вместо да правим.
```
int maxVal = 2*n - 1;
for (int val = 1; val <= maxVal; val += 2)
```
Може да правим:
```
for (int val = nextMin; val <= nextMax; val += 2)
```
Тази идея се връзва много добре със стратегията за редуването и прави кода малко по-бърз.

## Компилация
```
g++ *.cpp -DNDEBUG -DHAVE_PYTHON -O2
```
Ако се пуска върху posix compilant машина:
```
g++ *.cpp -DNDEBUG -DHAVE_PYTHON -DMAX_SECONDS=5 -O2
```
